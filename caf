#!/usr/bin/env python3
"""caf -- Calculation framework.

Usage:
    caf run [<remote>] [-p PROFILE] [-j N]
    caf process [--no-check]
    caf prepare
    caf extract
    caf init
    caf upload <remote>
    caf link <remote>
    caf submit <remote> [-p PROFILE] [-j N]
    caf parent add <path> [<name>]
    caf parent update <name>
    caf archive
    caf clean [<remote>]
    caf monitor <remote>
    caf check <remote>
    caf worker <id> <path>

Commands:

Options:
    -j N              Number of workers [default: 2].
    -p PROFILE        Job profile [default: default].
    --no-check        Don't verify data.
"""
from docopt import docopt

import json
import shutil
import sys
import subprocess
import signal
import pickle
from pathlib import Path
from configparser import ConfigParser

from caflib.Core import Result, Context, cd, mktmpdir, get_sha_dir, \
    sha_to_path, slugify


def prepare(ctx):
    if ctx.clean:
        if ctx.rundir.is_dir():
            return
    else:
        if ctx.rundir.is_dir():
            shutil.rmtree(str(ctx.rundir))
    ctx.rundir.mkdir(parents=True)
    ctx.prepare()
    task_db = []
    for param, calc in ctx.tasks:
        with mktmpdir(ctx.cache) as tmpdir:
            with cd(tmpdir):
                with open('command', 'w') as f:
                    f.write(calc.command)
                calc.prepare()
                sha_dir = get_sha_dir()
            path = ctx.cache/'objects'/sha_to_path(sha_dir)
            if not path.is_dir():
                if not path.parent.is_dir():
                    path.parent.mkdir(parents=True)
                shutil.move(tmpdir, str(path))
        stem = '_'.join('{}={}'.format(key, slugify(str(value)))
                        for key, value in param.items()) or '_'
        path_run = ctx.rundir/stem
        path_run.symlink_to(path if path.is_absolute() else Path('../..')/path)
        task_db.append((param, str(path_run)))
    with (ctx.rundir/'tasks.json').open('w') as f:
        json.dump(task_db, f, indent=4)


def init(ctx):
    subprocess.check_call('git checkout -b calc'.split())
    subprocess.check_call('git rm README.md'.split())
    subprocess.check_call('git commit -m'.split() + ['init calc'])


def run_local(ctx, profile):
    prepare(ctx)
    if ctx.cafdir:
        subprocess.call('{} {}'.format(ctx.cafdir/'worker_{}'.format(profile),
                                       ctx.rundir),
                        shell=True)
    else:
        subprocess.call('python3 -u caf worker 1 {0} 2>{0}/job.err | tee {0}/job.out'
                        .format(ctx.rundir),
                        shell=True)


def extract(ctx):
    if ctx.clean:
        if ctx.datadir.is_dir():
            return
    else:
        if ctx.datadir.is_dir():
            shutil.rmtree(str(ctx.datadir))
    if (ctx.rundir/'remote.json').is_file():
        extract_remote(ctx)
        return
    prepare(ctx)
    task_db = json.load((ctx.rundir/'tasks.json').open())
    for _, path in task_db:
        if not (Path(path)/'.lock'/'seal').is_file():
            print('Some tasks are not finished.')
            sys.exit(1)
    results = []
    for param, path in task_db:
        try:
            with cd(path):
                data = ctx.extract()
        except:
            print('info: Error occured in {}'.format(path))
            raise
        results.append((param, data))
    ctx.datadir.mkdir(parents=True)
    ctx.datalink.unlink()
    ctx.datalink.symlink_to(ctx.datadir)
    with (ctx.datadir/'data.p').open('wb') as f:
        pickle.dump(results, f, -1)


def process(ctx, check=True):
    if ctx.clean:
        if ctx.resultdir.is_dir():
            return
    if check:
        extract(ctx)
    if ctx.resultdir.is_dir():
        shutil.rmtree(str(ctx.resultdir))
    ctx.resultdir.mkdir(parents=True)
    with (ctx.datadir/'data.p').open('rb') as f:
        ctx.results = [Result(*x) for x in pickle.load(f)]
    with cd(ctx.resultdir):
        ctx.process()


def get_remote_cwd(ctx, remote):
    conf_str = subprocess.check_output(['ssh', remote, 'cat ~/.caf/config']).decode()
    conf = ConfigParser()
    conf.read_string(conf_str)
    return Path(conf['caf']['top'])/Path('.').resolve().relative_to(ctx.top)


def upload(ctx, remote, remote_cwd):
    print('Uploading to {}...'.format(remote))
    subprocess.check_call(['ssh', remote, 'mkdir -p {}'.format(remote_cwd)])
    subprocess.check_call(['rsync', '-ia',
                           '--exclude=.*',
                           '--exclude=build',
                           '--exclude=_cache',
                           '--exclude=*.pyc',
                           '--exclude=__pycache__',
                           '.',
                           '{}:{}'.format(remote, remote_cwd)])


def link(ctx, remote, remote_cwd):
    if ctx.rundir.is_dir():
        shutil.rmtree(str(ctx.rundir))
    ctx.rundir.mkdir(parents=True)
    with (ctx.rundir/'remote.json').open('w') as f:
        json.dump({'host': remote,
                   'path': str(remote_cwd)},
                  f, indent=4)


def run_remote(ctx, remote, remote_cwd, profile):
    upload(ctx, remote, remote_cwd)
    link(ctx, remote, remote_cwd)
    subprocess.check_call(
        ['ssh', remote,
         'cd {} && echo {} >HEAD && ./caf run -p {}'
         .format(remote_cwd, ctx.sha_repo, profile)])


def extract_remote(ctx):
    remote = json.load((ctx.rundir/'remote.json').open())
    upload(ctx, remote['host'], remote['path'])
    ret = subprocess.call(
        ['ssh', remote['host'],
         'cd {} && echo {} >HEAD && ./caf extract'
         .format(remote['path'], ctx.sha_repo)])
    if ret:
        return
    if not ctx.datadir.is_dir():
        ctx.datadir.mkdir(parents=True)
    subprocess.check_call(
        'rsync -ia {host}:{path}/{datalink}/ {datadir}/'
        .format(host=remote['host'],
                path=remote['path'],
                datalink=ctx.datalink,
                datadir=ctx.datadir).split())
    ctx.datalink.symlink_to(ctx.datadir)


def submit(remote, job):
    pass


def worker(myid, path):
    def sigint_handler(sig, frame):
        print('Worker {} interrupted, aborting.'.format(myid))
        sys.exit()
    signal.signal(signal.SIGINT, sigint_handler)

    print('Worker {} alive and ready.'.format(myid))
    tasks = path.glob('*/command')
    while True:
        try:
            task = next(tasks).parent
        except StopIteration:
            break
        name = task.name
        lock = task/'.lock'
        try:
            lock.mkdir()
        except OSError:
            continue
        print('Worker {} started working on {}...'.format(myid, name))
        with cd(task):
            with open('run.out', 'w') as stdout, \
                    open('run.err', 'w') as stderr:
                subprocess.check_call(open('command').read(),
                                      shell=True,
                                      stdout=stdout,
                                      stderr=stderr)
        (lock/'seal').touch()
        print('Worker {} finished working on {}.'.format(myid, name))
    print('Worker {} has no more tasks to do, aborting.'.format(myid))
    sys.exit()


if __name__ == '__main__':
    args = docopt(__doc__)
    if args['worker']:
        worker(args['<id>'], Path(args['<path>']))
    ctx = Context()
    remote = args['<remote>']
    if remote:
        remote_cwd = get_remote_cwd(ctx, remote)
    profile = args['-p']
    if args['init']:
        init(ctx)
    if args['prepare']:
        prepare(ctx)
    if args['run']:
        if remote:
            run_remote(ctx, remote, remote_cwd, profile)
        else:
            run_local(ctx, profile)
    if args['link']:
        link(ctx, remote, remote_cwd)
    if args['extract']:
        extract(ctx)
    if args['process']:
        process(ctx, not args['--no-check'])
    if args['upload']:
        upload(ctx, remote, remote_cwd)
    # if args['submit']:
    #     submit(ctx.remote, ctx.job)


# archive_%:
# 	@${MAKE} --no-print-directory results_$*/$(notdir ${PWD}).tar.gz
#
# # TODO
# results_%/$(notdir ${PWD}).tar.gz:
# 	@echo "Creating archive $@..."
#
# clean:
# ifneq ("$(wildcard *.pyc)", "")
# 	rm *.pyc
# endif
#
# cleanrun:
# ifneq ("$(wildcard RUN)", "")
# 	rm -r RUN
# endif
#
# # TODO
# distclean: clean cleanrun
#
# monitor_%:
# 	@ssh $* qmy
#
# check: numoftasks
#
# numoftasks:
# 	@echo "Number of initialized tasks: $(shell ls -d RUN/*.start 2>/dev/null | wc -l)"
# 	@echo "Number of running tasks: $(shell ls -d RUN/*.running.* 2>/dev/null | wc -l)"
# 	@echo "Number of finished tasks: $(shell ls -d RUN/*.done 2>/dev/null | wc -l)"
#
# check_%:
# 	@echo "Connecting to $*..."
# 	@ssh $* "cd ${remotedir} && make check"
#
# cleanrun_%:
# 	@echo "Connecting to $*..."
# 	@ssh $* "cd ${remotedir} && make cleanrun"
#
# distclean_%:
# 	@echo "Connecting to $*..."
# 	@ssh $* "cd ${remotedir} && make distclean"
