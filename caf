#!/usr/bin/env python
"""caf -- Calculation framework.

Usage:
    caf prepare
    caf worker <id> <path>
    caf extract
    caf process
    caf upload <remote>
    caf run [-j N] local
    caf run [-j N] <remote> <job>
    caf submit <remote> <job>
    caf parent add <path> [<name>]
    caf parent update <name>
    caf archive
    caf clean [<remote>]
    caf distclean [<remote>]
    caf monitor <remote>
    caf check <remote>
    caf init

Commands:

Options:
    -j N     Number of workers [default: 2].
"""
from docopt import docopt
from pathlib import Path
import imp
from collections import namedtuple
from slugify import slugify
import json
import hashlib
import sys
import subprocess
import signal
from git import Repo
from multiprocessing import Process
import cPickle as pickle


sys.path.append('tools')
cscript = imp.new_module('cscript')
exec(open('cscript').read(), cscript.__dict__)


class Context(object):
    def __init__(self, args):
        self.remote = args['<remote>']
        self.id = args['<id>']
        self.job = args['<job>']
        path = args['<path>']
        self.path = Path(path) if path else None
        self.inputdir = Path(getattr(cscript, 'inputdir', 'INPUTS'))
        self.rundir = Path(getattr(cscript, 'rundir', 'RUNS'))
        self.resultdir = Path(getattr(cscript, 'resultdir', 'RESULTS'))
        root = Path(cscript.root).resolve()
        self.remotedir = Path(cscript.remoteroot)/Path('.').resolve().relative_to(root)
        repo = Repo('.')
        head = repo.head.commit
        self.sharepo = (head.NULL_HEX_SHA if head.diff(None) else head.hexsha)[:7]


Parameter = namedtuple('Parameter', ['key', 'value', 'str'])
Parameter.__new__.__defaults__ = (None,)
Row = namedtuple('Row', ['key', 'value'])


def prepare(inputdir):
    tasks, preparer = cscript.prepare()
    tasks = [[Parameter(*p) for p in t] for t in tasks]
    for task in tasks:
        stem = '_'.join(slugify(unicode(p.str)) for p in task if p.str is not None) or '_'
        path = inputdir/(stem + '.start')
        path.mkdir(parents=True)
        preparer(path, {p.key: p.value for p in task if p.value is not None})
        with open(str(path/'info.json'), 'w') as f:
            json.dump({p.key: p.str for p in task if p.str is not None}, f)


def worker(myid, path):
    def sigint_handler(sig, frame):
        print('Worker {} interrupted, aborting.'.format(myid))
        sys.exit()
    signal.signal(signal.SIGINT, sigint_handler)

    print('Worker {} alive and ready.'.format(myid))
    while True:
        tasks = path.glob('*.start')
        try:
            startpath = next(tasks)
        except StopIteration:
            break
        name = startpath.stem
        runpath = startpath.with_suffix('.%s.running' % myid)
        try:
            startpath.rename(runpath)
        except:
            continue
        print('Worker {} started working on {}...'.format(myid, name))
        with (runpath/'run.log').open('w') as stdout, \
                (runpath/'run.err').open('w') as stderr:
            subprocess.check_call(str((runpath/'run').resolve()),
                                  stdout=stdout,
                                  stderr=stderr,
                                  cwd=str(runpath))
        runpath.rename(startpath.with_suffix('.done'))
        print('Worker {} finished working on {}.'.format(myid, name))
    print('Worker {} has no more tasks to do, aborting.'.format(myid))


def runlocal(inputdir, rundir, sharepo):
    h = hashlib.new('sha1')
    h.update(subprocess.check_output(['cat']
                                     + [str(p) for p in inputdir.glob('*/**/*')]))
    shainputs = h.hexdigest()[:7]
    rundir.mkdir(parents=True)
    (rundir/sharepo).symlink_to(shainputs)
    if not (rundir/shainputs).is_dir():
        inputdir.rename(rundir/shainputs)
        pool = []
        for i in range(int(args['-j'])):
            p = Process(target=worker, args=(i, rundir/shainputs))
            p.start()
            pool.append(p)
        for p in pool:
            try:
                p.join()
            except KeyboardInterrupt:
                pass


def extract(rundir, resultdir, sharepo):
    results = []
    for path in (rundir/sharepo).glob('*.done'):
        with open(str(path/'info.json')) as f:
            key = json.load(f)
        try:
            value = cscript.extract(path)
        except:
            print('info: Error occured in {}'.format(path))
            raise
        results.append((key, value))
    (resultdir/sharepo).mkdir(parents=True)
    with (resultdir/sharepo/'results.p').open('wb') as f:
        pickle.dump(results, f, -1)


def process(resultdir, sharepo):
    with(resultdir/sharepo/'results.p').open('rb') as f:
        results = [Row(*t) for t in pickle.load(f)]
    cscript.process(resultdir/sharepo, results)


def upload(remote, remotedir):
    print('Uploading to {}...'.format(remote))
    subprocess.check_call(['ssh', remote, 'mkdir -p %s' % remotedir])
    subprocess.check_call(['rsync', '-ia',
                           '--exclude=*.pyc',
                           '--exclude=INPUTS',
                           '--exclude=.*',
                           '--include=%s_*.job.sh' % remote,
                           '--exclude=*_*.job.sh',
                           '.',
                           '%s:%s' % (remote, remotedir)])


def runremote(remote, remotedir, job):
    subprocess.check_call(['ssh', remote,
                           'cd %s && ./caf prepare' % remotedir])
    subprocess.check_call(['ssh', remote,
                           'cd %s && ./caf submit %s %s' % (remotedir, remote, job)])


def submit(remote, job):
    getattr(cscript, 'remote_%s_%s' % (remote, job))()


class ArrayEncoder(json.JSONEncoder):
    def default(self, obj):
        try:
            return obj.tolist()
        except AttributeError:
            return super().default(obj)


if __name__ == '__main__':
    args = docopt(__doc__)
    ctx = Context(args)
    if args['prepare']:
        prepare(ctx.inputdir)
    if args['worker']:
        worker(ctx.id, ctx.path)
    if args['run'] and args['local']:
        runlocal(ctx.inputdir, ctx.rundir, ctx.sharepo)
    if args['extract']:
        extract(ctx.rundir, ctx.resultdir, ctx.sharepo)
    if args['process']:
        process(ctx.resultdir, ctx.sharepo)
    if args['upload']:
        upload(ctx.remote, ctx.remotedir)
    if args['run'] and args['<remote>']:
        upload(ctx.remote, ctx.remotedir)
        runremote(ctx.remote, ctx.remotedir, ctx.job)
    if args['submit']:
        submit(ctx.remote, ctx.job)


# if scratch:
#     today = time.strftime('%y-%m-%d')
#     rundir = os.path.join(scratch, today, myid, basename)
#     os.makedirs(rundir)
#     os.symlink(rundir, os.path.join(runname, 'rundir'))
# else:


# $(addprefix results_%/,${outputs}): results_%/results.p process.py
# 	cd results_$* && python ../process.py
#
# results_%/results.p: RUN/%_job.log extract.py
# ifneq ("$(wildcard RUN/*.start RUN/*.running.*)", "")
# 	$(error "Some jobs are still running.")
# endif
# 	python extract.py
# 	mkdir -p results_$* && mv RUN/results.p $@
#
# results_%/seal:
# # find RUN -path "*.done/rundir/*" ! -name "run.*" | xargs cat | shasum
# # find RUN -path "*.done/*" \( -name rundir -prune -o -print \) | xargs cat | shasum
#
# RUN/%_job.log: prepare.py ${inputs} | checkifremote_%
# 	@${MAKE} --no-print-directory prepare
# 	@${MAKE} --no-print-directory run_$*
# ifdef REMOTE
# 	@${MAKE} --no-print-directory print_error
# endif
#
# print_error:
# 	$(error "Wait till the job finishes, then run make again.")
#
# checkifremote_local: ;
# checkifremote_%: ;
# ifndef REMOTE
# 	$(error "Trying to run remote on local.")
# endif
#
# run_%:
# 	bash ~/bin/submit.sh $*.job.sh
# 	@sleep 1  # some submitters print asynchronously
#
# # TODO
# update:
# 	@echo "Updating tools..."
#
# remote_%: upload_$$(firstword $$(subst _, , %))
# ifdef OFFLINE
# 	@echo "Skipping download."
# else
# 	$(eval remote := $(firstword $(subst _, ,$*)))
# 	@echo "Connecting to ${remote}..."
# 	@ssh ${remote} "cd ${remotedir} && make results_$*/results.p REMOTE=1"
# 	@echo "Downloading results from ${remote}..."
# 	@rsync -ia ${remote}:${remotedir}/results_$*/results.p results_$*/
# endif
# 	@${MAKE} --no-print-directory $(addprefix results_$*/,${outputs})
#
# upload_%: # TODO ???
# ifdef OFFLINE
# 	@echo "Skipping upload."
# else
# 	@echo "Uploading to $*..."
# 	@ssh $* "mkdir -p ${remotedir}"
# 	@rsync -ia \
# 		--exclude=*.pyc --exclude=RUN $(addprefix --exclude=,${excluded}) \
# 		--include=$*_*.job.sh --exclude=*_*.job.sh \
# 		--exclude=results_* \
# 		${PWD}/* $*:${remotedir}/
# endif
#
# submit_%:
# 	$(eval remote := $(firstword $(subst _, ,$*)))
# 	@echo "Connecting to ${remote}..."
# 	@ssh ${remote} "cd ${remotedir} && make run_$*"
#
# archive_%:
# 	@${MAKE} --no-print-directory results_$*/$(notdir ${PWD}).tar.gz
#
# # TODO
# results_%/$(notdir ${PWD}).tar.gz:
# 	@echo "Creating archive $@..."
#
# clean:
# ifneq ("$(wildcard *.pyc)", "")
# 	rm *.pyc
# endif
#
# cleanrun:
# ifneq ("$(wildcard RUN)", "")
# 	rm -r RUN
# endif
#
# # TODO
# distclean: clean cleanrun
#
# monitor_%:
# 	@ssh $* qmy
#
# check: numoftasks
#
# numoftasks:
# 	@echo "Number of initialized tasks: $(shell ls -d RUN/*.start 2>/dev/null | wc -l)"
# 	@echo "Number of running tasks: $(shell ls -d RUN/*.running.* 2>/dev/null | wc -l)"
# 	@echo "Number of finished tasks: $(shell ls -d RUN/*.done 2>/dev/null | wc -l)"
#
# check_%:
# 	@echo "Connecting to $*..."
# 	@ssh $* "cd ${remotedir} && make check"
#
# cleanrun_%:
# 	@echo "Connecting to $*..."
# 	@ssh $* "cd ${remotedir} && make cleanrun"
#
# distclean_%:
# 	@echo "Connecting to $*..."
# 	@ssh $* "cd ${remotedir} && make distclean"
