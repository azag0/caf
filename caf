#!/usr/bin/env python3
"""caf -- Calculation framework.

Usage:
    caf upload <remote>
    caf prepare [<remote>]
    caf link <remote>
    caf run [-p PROFILE] [-j N]
    caf check
    caf extract
    caf download <remote>
    caf process
    caf archive
    caf worker <id> <path>

Commands:

Options:
    -j N              Number of workers [default: 1].
    -p PROFILE        Job profile [default: default].
    --no-check        Don't verify data.
"""
from docopt import docopt

import json
import shutil
import sys
import subprocess
import signal
import pickle
from pathlib import Path
from configparser import ConfigParser
from collections import namedtuple

from caflib.Core import Result, Context, cd, mktmpdir, get_sha_dir, \
    sha_to_path, slugify


Remote = namedtuple('Remote', ['host', 'top'])


def upload(ctx, remote):
    print('Uploading to {}...'.format(remote.host))
    subprocess.check_call(['ssh', remote.host,
                           'mkdir -p {0.top} && cd {0.top} && echo {1} >HEAD'
                           .format(remote, ctx.sha_repo)])
    subprocess.check_call(['rsync', '-ia',
                           '--exclude=.*',
                           '--exclude=build',
                           '--exclude=_cache',
                           '--exclude=*.pyc',
                           '--exclude=__pycache__',
                           '.',
                           '{0.host}:{0.top}'.format(remote)])


def prepare(ctx):
    if ctx.rundir.is_dir():
        if ctx.clean:
            print('Runs are prepared')
            return
        else:
            shutil.rmtree(str(ctx.rundir))
    ctx.rundir.mkdir(parents=True)
    ctx.prepare()
    task_db = []
    for param, calc in ctx.tasks:
        with mktmpdir(ctx.cache) as tmpdir:
            with cd(tmpdir):
                with open('command', 'w') as f:
                    f.write(calc.command)
                calc.prepare()
                sha_dir = get_sha_dir()
            path = ctx.cache/'objects'/sha_to_path(sha_dir)
            if not path.is_dir():
                if not path.parent.is_dir():
                    path.parent.mkdir(parents=True)
                shutil.move(tmpdir, str(path))
        stem = '_'.join('{}={}'.format(key, slugify(str(value)))
                        for key, value in param.items()) or '_'
        path_run = ctx.rundir/stem
        path_run.symlink_to(path if path.is_absolute() else Path('../..')/path)
        task_db.append((param, str(path_run)))
    with (ctx.rundir/'tasks.json').open('w') as f:
        json.dump(task_db, f, indent=4)


def link(ctx, remote):
    if ctx.rundir.is_dir():
        if ctx.clean:
            print('Runs are prepared')
            return
        else:
            shutil.rmtree(str(ctx.rundir))
    ctx.rundir.mkdir(parents=True)
    with (ctx.rundir/'remote.json').open('w') as f:
        json.dump((remote.host, str(remote.top)), f, indent=4)


def run(ctx, profile):
    if not (ctx.rundir/'tasks.json').is_file():
        raise RuntimeError('Runs have not been prepared')
    subprocess.call('{} {}'.format(ctx.cafdir/'worker_{}'.format(profile),
                                   ctx.rundir),
                    shell=True)


def check(ctx):
    n_all = len(json.load((ctx.rundir/'tasks.json').open()))
    n_locked = len(list(ctx.rundir.glob('*/.lock')))
    n_sealed = len(list(ctx.rundir.glob('*/.lock/seal')))
    print('Number of initialized tasks: {}'.format(n_all))
    print('Number of running tasks: {}'.format(n_locked-n_sealed))
    print('Number of finished tasks: {}'.format(n_sealed))


def extract(ctx):
    if not (ctx.rundir/'tasks.json').is_file():
        raise RuntimeError('Runs have not been prepared')
    task_db = json.load((ctx.rundir/'tasks.json').open())
    for _, path in task_db:
        if not (Path(path)/'.lock'/'seal').is_file():
            raise RuntimeError('Some tasks are not finished')
    if ctx.datadir.is_dir():
        if ctx.clean:
            print('Data are extracted')
            return
        else:
            shutil.rmtree(str(ctx.datadir))
    ctx.datadir.mkdir(parents=True)
    results = []
    for param, path in task_db:
        try:
            with cd(path):
                data = ctx.extract()
        except:
            print('info: Error occured in {}'.format(path))
            raise
        results.append((param, data))
    with (ctx.datadir/'data.p').open('wb') as f:
        pickle.dump(results, f, -1)
    if ctx.datalink.is_symlink():
        ctx.datalink.unlink()
    ctx.datalink.symlink_to(ctx.datadir)


def download(ctx, remote):
    print('Downloading from {}...'.format(remote.host))
    if ctx.datadir.is_dir():
        if ctx.clean:
            print('Data are extracted')
            return
        else:
            shutil.rmtree(str(ctx.datadir))
    ctx.datadir.mkdir(parents=True)
    subprocess.check_call('rsync -ia {.host}:{.top}/{}/ {}/'
                          .format(remote, ctx.datalink, ctx.datadir).split())
    if ctx.datalink.is_symlink():
        ctx.datalink.unlink()
    ctx.datalink.symlink_to(ctx.datadir)


def process(ctx):
    if ctx.resultdir.is_dir():
        if ctx.clean:
            print('Data are processed')
            return
        else:
            shutil.rmtree(str(ctx.resultdir))
    ctx.resultdir.mkdir(parents=True)
    with (ctx.datadir/'data.p').open('rb') as f:
        ctx.results = [Result(*x) for x in pickle.load(f)]
    with cd(ctx.resultdir):
        ctx.process()


def worker(myid, path):
    def sigint_handler(sig, frame):
        print('Worker {} interrupted, aborting.'.format(myid))
        sys.exit()
    signal.signal(signal.SIGINT, sigint_handler)

    print('Worker {} alive and ready.'.format(myid))
    tasks = path.glob('*/command')
    while True:
        try:
            task = next(tasks).parent
        except StopIteration:
            break
        name = task.name
        lock = task/'.lock'
        try:
            lock.mkdir()
        except OSError:
            continue
        print('Worker {} started working on {}...'.format(myid, name))
        with cd(task):
            with open('run.out', 'w') as stdout, \
                    open('run.err', 'w') as stderr:
                subprocess.check_call(open('command').read(),
                                      shell=True,
                                      stdout=stdout,
                                      stderr=stderr)
        (lock/'seal').touch()
        print('Worker {} finished working on {}.'.format(myid, name))
    print('Worker {} has no more tasks to do, aborting.'.format(myid))


def _get_remote_top(ctx, host):
    conf_str = subprocess.check_output(['ssh', host, 'cat ~/.caf/config']).decode()
    conf = ConfigParser()
    conf.read_string(conf_str)
    return Path(conf['caf']['top'])/Path('.').resolve().relative_to(ctx.top)


def _remote_cmd(remote, cmd):
    print('Connecting to {.host}...'.format(remote))
    subprocess.check_call(['ssh', remote.host,
                           'cd {.top} && ./caf {}'.format(remote, cmd)])


if __name__ == '__main__':
    args = docopt(__doc__)
    if args['worker']:
        worker(args['<id>'], Path(args['<path>']))
        sys.exit()
    ctx = Context()
    remote = args['<remote>']
    if remote:
        remote = Remote(remote, _get_remote_top(ctx, remote))
    if args['upload']:
        upload(ctx, remote)
    elif args['prepare']:
        if remote:
            upload(ctx, remote)
            _remote_cmd(remote, 'prepare')
            link(ctx, remote)
        else:
            prepare(ctx)
    elif args['link']:
        link(ctx, remote)
    else:
        try:
            remote = json.load((ctx.rundir/'remote.json').open())
            remote = Remote(remote[0], Path(remote[1]))
        except FileNotFoundError:
            remote = None
        if args['run']:
            if remote:
                _remote_cmd(remote, 'run -p {} -j {}'.format(args['-p'], args['-j']))
            else:
                for _ in range(int(args['-j'])):
                    run(ctx, args['-p'])
        elif args['check']:
            if remote:
                subprocess.check_call(['ssh', remote.host, 'qmy'])
                _remote_cmd(remote, 'check')
            else:
                check(ctx)
        elif args['extract']:
            if remote:
                _remote_cmd(remote, 'extract')
                download(remote)
            else:
                extract(ctx)
        elif args['process']:
            process(ctx)
